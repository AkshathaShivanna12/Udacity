{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Passing data to csv files\n",
    "\n",
    "In this notebook we document the process of cleaning and passing the data to csv files for the project analyzing open street map data of the city of Lima, Peru.\n",
    "\n",
    "In the repository, it is possible to find the file \"lima_sample.osm\" which contains a small subset of the XML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper functions. This functions help us to take data from the XML document.\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.cElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "from collections import Counter\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "OSM_FILE = \"lima_peru.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"lima_sample.osm\"\n",
    "\n",
    "k = 50 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"lima_sample.osm\"):\n",
    "    with open(SAMPLE_FILE, 'wb') as output:\n",
    "        output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        output.write('<osm>\\n  ')\n",
    "    \n",
    "        # Write every kth top level element\n",
    "        for i, element in enumerate(get_element(OSM_FILE)):\n",
    "            if i % k == 0:\n",
    "                output.write(ET.tostring(element, encoding='utf-8'))\n",
    "    \n",
    "        output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the process of analizing our data by checking the street names. This is useful to see if the data present some irregularities and to implement solutions of the observed problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Getting street names\n",
    "def get_streets(element):\n",
    "    \"\"\"Helper function to get street names from data \"\"\"\n",
    "    for tag in element.findall('tag'):\n",
    "        if tag.attrib['k'].startswith(\"addr:street\"):\n",
    "            street_name = tag.attrib['v']\n",
    "            return street_name\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_sample = []\n",
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    street = get_streets(element)\n",
    "    if  street is not None:\n",
    "        street_sample.append(street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Malec\\xf3n',\n",
       " u'\\xd3valo',\n",
       " u'Avenida',\n",
       " 'Avenida',\n",
       " 'Los',\n",
       " 'Avenida',\n",
       " 'Coricancha',\n",
       " 'Las',\n",
       " 'Satelite',\n",
       " 'Jorge']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item.split()[0] for item in street_sample[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can notice from this is that we might have problems trying to aggregate the data because some entities are with accents and some not. We decide to uniform the data by removing all accents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "    \"\"\"Strips all the accents from unicode data\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizing_attributes(encode_attrib):\n",
    "    \"\"\"Pass the data to the appropriate unicode format and then accents are removed\"\"\"\n",
    "    if encode_attrib is not None:\n",
    "        atrib_encode = encode_attrib.encode(\"utf-8\")\n",
    "        attr_cont = atrib_encode.decode(\"utf-8\")\n",
    "        return strip_accents(attr_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Malecon de la Reserva',\n",
       " u'Ovalo Gutierrez',\n",
       " u'Avenida Nicolas de Pierola',\n",
       " u'Avenida Proceres de la Independencia',\n",
       " u'Los Cedros',\n",
       " u'Avenida Circunvalacion',\n",
       " u'Coricancha',\n",
       " u'Las Lausonias',\n",
       " u'Satelite',\n",
       " u'Jorge Chavez']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if function works appropriately\n",
    "#Checking first 20 entries\n",
    "map(normalizing_attributes, street_sample)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have uniformly treated the names of the streets, we want to check further problems with the data. Note that the first name of the street names gives us its type (e.g. Avenida, Jiron, Pasaje). With this information, we want to chek if there are certain problems with these entries that we can correct. In what follows we generate a function to get the first word of the street entry. We then count common entries by using the counter module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getting_norm_and_type_street_names(street_name):\n",
    "    \"\"\"Gets fisrt word of street - For checking type\"\"\"\n",
    "    norm_street = None\n",
    "    street_type = None\n",
    "    #street_name = get_streets(element)\n",
    "    #\n",
    "    norm_street = normalizing_attributes(street_name)\n",
    "    if street_name:\n",
    "        street_type = norm_street.split()[0].strip()\n",
    "        norm_street = norm_street\n",
    "    return street_type, norm_street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type_street_counter = Counter(getting_norm_and_type_street_names(get_streets(element))[0] for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({None: 13273,\n",
       "         u'13': 1,\n",
       "         u'300': 1,\n",
       "         u'7': 1,\n",
       "         u'Alameda': 4,\n",
       "         u'Alfredo': 2,\n",
       "         u'Ampliacion': 1,\n",
       "         u'Arequipa': 2,\n",
       "         u'Auxiliar': 3,\n",
       "         u'Av.': 6,\n",
       "         u'Avenida': 288,\n",
       "         u'Callao': 1,\n",
       "         u'Calle': 40,\n",
       "         u'Carretera': 5,\n",
       "         u'Casma': 1,\n",
       "         u'Colcabamba': 1,\n",
       "         u'Coricancha': 1,\n",
       "         u'Federico': 1,\n",
       "         u'Gallesi': 1,\n",
       "         u'General': 1,\n",
       "         u'Gonzales': 1,\n",
       "         u'Gozzoli': 1,\n",
       "         u'Gran': 1,\n",
       "         u'Guardia': 1,\n",
       "         u'Guillermo': 1,\n",
       "         u'Hernando': 1,\n",
       "         u'Huancarama': 1,\n",
       "         u'Inca': 1,\n",
       "         u'Iquique': 1,\n",
       "         u'Jesus': 1,\n",
       "         u'Jiron': 48,\n",
       "         u'Jorge': 3,\n",
       "         u'Jose': 2,\n",
       "         u'Jr.': 1,\n",
       "         u'La': 2,\n",
       "         u'Las': 9,\n",
       "         u'Leoncio': 1,\n",
       "         u'Los': 10,\n",
       "         u'Malecon': 1,\n",
       "         u'Manuel': 2,\n",
       "         u'Maria': 1,\n",
       "         u'Mercedes': 1,\n",
       "         u'Monterrosa': 1,\n",
       "         u'Ovalo': 1,\n",
       "         u'Pacasmayo': 1,\n",
       "         u'Pasaje': 3,\n",
       "         u'Paseo': 1,\n",
       "         u'Paz': 1,\n",
       "         u'Prolongacion': 4,\n",
       "         u'Puente': 2,\n",
       "         u'Rio': 1,\n",
       "         u'Rufino': 1,\n",
       "         u'San': 2,\n",
       "         u'Santa': 1,\n",
       "         u'Satelite': 1,\n",
       "         u'Sergio': 1,\n",
       "         u'Tupac': 1,\n",
       "         u'Victor': 3,\n",
       "         u'maynas': 1})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_street_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list, we identify four problems:\n",
    "1. Av., Avenida and Avienda: They all refer to the same stret type.\n",
    "2. Defensores and Defendores: This might be a typing issue.\n",
    "3. Jr., Jr, Jiron: They all refere to the same street type.\n",
    "4. Calle and calle: difference because of capitalization.\n",
    "We correct this discrepancies in the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corrections_street_type(street_type):\n",
    "    \"\"\"changing types according to corrections\"\"\"\n",
    "    if street_type in [\"Av\", \"Av.\", \"Avienda\"]:\n",
    "        correct_type = \"Avenida\"\n",
    "    elif street_type in [\"Defendores\"]:\n",
    "        correct_type = \"Defensores\"\n",
    "    elif street_type in [\"Jr\", \"Jr.\"]:\n",
    "        correct_type = \"Jiron\"\n",
    "    elif street_type in [\"calle\"]:\n",
    "        correct_type = \"Calle\"\n",
    "    else:\n",
    "        correct_type = None\n",
    "    return correct_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correcting_street_names(street_name):\n",
    "    \"\"\"\"Corrects problematic entries according to specified rule in function\n",
    "    corrections_street_type\"\"\"\n",
    "    street_name, street_type = getting_norm_and_type_street_names(street_name)\n",
    "    corrected_name = corrections_street_type(street_type)\n",
    "    if corrected_name:\n",
    "        street_name = street_name.replace(street_type, corrected_name, 1)\n",
    "    return street_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print only the street types for our data set to see if changes were correctly applied. As it is possible to see, all changes specified by the function yield the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_type_checker = set()\n",
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    street_name = get_streets(element)\n",
    "    if street_name:\n",
    "        street_type_checker.add(correcting_street_names(street_name).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'13',\n",
       " u'300',\n",
       " u'7',\n",
       " u'Alameda',\n",
       " u'Alfredo',\n",
       " u'Ampliacion',\n",
       " u'Arequipa',\n",
       " u'Auxiliar',\n",
       " u'Av.',\n",
       " u'Avenida',\n",
       " u'Callao',\n",
       " u'Calle',\n",
       " u'Carretera',\n",
       " u'Casma',\n",
       " u'Colcabamba',\n",
       " u'Coricancha',\n",
       " u'Federico',\n",
       " u'Gallesi',\n",
       " u'General',\n",
       " u'Gonzales',\n",
       " u'Gozzoli',\n",
       " u'Gran',\n",
       " u'Guardia',\n",
       " u'Guillermo',\n",
       " u'Hernando',\n",
       " u'Huancarama',\n",
       " u'Inca',\n",
       " u'Iquique',\n",
       " u'Jesus',\n",
       " u'Jiron',\n",
       " u'Jorge',\n",
       " u'Jose',\n",
       " u'Jr.',\n",
       " u'La',\n",
       " u'Las',\n",
       " u'Leoncio',\n",
       " u'Los',\n",
       " u'Malecon',\n",
       " u'Manuel',\n",
       " u'Maria',\n",
       " u'Mercedes',\n",
       " u'Monterrosa',\n",
       " u'Ovalo',\n",
       " u'Pacasmayo',\n",
       " u'Pasaje',\n",
       " u'Paseo',\n",
       " u'Paz',\n",
       " u'Prolongacion',\n",
       " u'Puente',\n",
       " u'Rio',\n",
       " u'Rufino',\n",
       " u'San',\n",
       " u'Santa',\n",
       " u'Satelite',\n",
       " u'Sergio',\n",
       " u'Tupac',\n",
       " u'Victor',\n",
       " u'maynas'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_type_checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Phone data\n",
    "\n",
    "We implement a function to check if phone data is correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phone(element):\n",
    "    \"\"\"Helper function to get all the available phones in the data set.\"\"\"\n",
    "    phone = None\n",
    "    for tag in element.findall('tag'):\n",
    "        if tag.attrib['k'] == 'phone':\n",
    "            phone = tag.attrib['v']\n",
    "    return phone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def phone_cleaner(phone):\n",
    "    \"\"\"Checks if entry constitutes a valid phone and uniformizes data.\n",
    "    >>>phone_cleaner(\"(+51) 01 3766489\")\n",
    "    '+51 1 3766489'\n",
    "    >>>phone_cleaner(\"(+511 4612344 Anexo 25\")\n",
    "    '+51 1 4612344 ext. 25'\n",
    "    \"\"\"\n",
    "    if phone:\n",
    "        phone_list = re.split(\"[;\\,\\/]\", phone)\n",
    "        phone = []\n",
    "        for number in phone_list:\n",
    "            # Taking out 511 in the beginning\n",
    "            matching = re.sub(r\"^\\s?\\(?\\+?\\s?5\\s?1\\)?\\s?1?\\s?\\)?\", \"\", number) \n",
    "            #Taking out (01) and - and white spaces\n",
    "            matching_2 = re.sub(r\"^\\(?\\-?0?\\-?1\\)?|-|\\s|\\.|\\(|\\)\", \"\", matching) \n",
    "            taking_ext = re.split(r\"Anex[os]|ex[t\\.]\", matching_2, flags=re.IGNORECASE)\n",
    "            extension_num = None\n",
    "            if len(taking_ext)==2:\n",
    "                extension_num = taking_ext[1]\n",
    "            #Checking if phones are only composed of numbers\n",
    "            if re.match(r\"\\d\", taking_ext[0]): \n",
    "                #Checking if has exact digits to be a cell phone or fixed line\n",
    "                if len(taking_ext[0]) == 9:\n",
    "                    cell_number = \"+51 \" + taking_ext[0]\n",
    "                    phone.append(cell_number)\n",
    "                elif len(taking_ext[0]) == 7:\n",
    "                    fix_num = \"+51 1 \" + taking_ext[0]\n",
    "                    if extension_num and len(extension_num) <= 4:\n",
    "                        fix_num += \" \" \"ext. \" + extension_num\n",
    "                    phone.append(fix_num)\n",
    "        return phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+51 1 6104000 ['+51 1 6104000']\n",
      "+51 1 5192000 ['+51 1 5192000']\n",
      "+51 1 6197000 ['+51 1 6197000']\n",
      "+51 1 3563916 ['+51 1 3563916']\n",
      "+51 1 211 8800 (ext. 2233) ['+51 1 2118800 ext. 2233']\n",
      "012191000 ['+51 1 2191000']\n",
      "6610446 ['+51 1 6610446']\n",
      "4632727 ['+51 1 4632727']\n",
      "+51980910194 ['+51 980910194']\n",
      "+515290535 ['+51 1 5290535']\n",
      "+51 1 4428828 ['+51 1 4428828']\n",
      "012913613 ['+51 1 2913613']\n",
      "+51 1 4596890 ['+51 1 4596890']\n",
      "(511) 433-4724 / (511) 424-1988 ['+51 1 4334724', '+51 1 4241988']\n",
      "634-2700 ['+51 1 6342700']\n",
      "418 0710 ['+51 1 4180710']\n",
      "424 6634 ['+51 1 4246634']\n",
      "2762753 ['+51 1 2762753']\n"
     ]
    }
   ],
   "source": [
    "#Checking function / This function was already tested with all the data set. here just with sample file\n",
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    if get_phone(element):\n",
    "        print get_phone(element), phone_cleaner(get_phone(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking amenities and tag contents for banks and fast food.\n",
    "\n",
    "We wanted to make a more rigourous analysis of banks and fast foods in the data. We started checking what are the available amenity tags. From there we went to analyzing what type of data is associated with banks and fast food. We observed and try to correct problems realted with multiple names for a single entities and cases of bad tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_amenity(element):\n",
    "    \"\"\"gets name of amenities of a given element\"\"\"\n",
    "    amen_name = None\n",
    "    type_amen = None\n",
    "    for tag in element.findall('tag'):\n",
    "        if tag.attrib['k'] == 'name':\n",
    "                amen_name = tag.attrib['v']\n",
    "        if tag.attrib['k'] =='amenity':\n",
    "            type_amen = tag.attrib['v']\n",
    "    return (normalizing_attributes(type_amen), normalizing_attributes(amen_name))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types_amen = set()\n",
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    types_amen.add(get_amenity(element)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None,\n",
       " u'bank',\n",
       " u'bar',\n",
       " u'bench',\n",
       " u'bureau_de_change',\n",
       " u'bus_station',\n",
       " u'cafe',\n",
       " u'car_rental',\n",
       " u'car_wash',\n",
       " u'casino',\n",
       " u'childcare',\n",
       " u'cinema',\n",
       " u'clinic',\n",
       " u'college',\n",
       " u'community_centre',\n",
       " u'courthouse',\n",
       " u'dentist',\n",
       " u'doctors',\n",
       " u'embassy',\n",
       " u'events_venue',\n",
       " u'fast_food',\n",
       " u'fire_station',\n",
       " u'fountain',\n",
       " u'fuel',\n",
       " u'hospital',\n",
       " u'kindergarten',\n",
       " u'marketplace',\n",
       " u'nightclub',\n",
       " u'parking',\n",
       " u'parking_entrance',\n",
       " u'pharmacy',\n",
       " u'place_of_worship',\n",
       " u'police',\n",
       " u'public_building',\n",
       " u'ranger_station',\n",
       " u'restaurant',\n",
       " u'school',\n",
       " u'social_centre',\n",
       " u'studio',\n",
       " u'swimming_pool',\n",
       " u'telephone',\n",
       " u'theatre',\n",
       " u'toilets',\n",
       " u'veterinary',\n",
       " u'waste_basket'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_amen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bank_names = set()\n",
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    type_amen, amen_name = get_amenity(element)\n",
    "    if type_amen == 'bank':\n",
    "            bank_names.add(amen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None,\n",
       " u'BBVA',\n",
       " u'BCP',\n",
       " u'BanBif',\n",
       " u'Banbif',\n",
       " u'Banco Falabella',\n",
       " u'Banco Financiero',\n",
       " u'Banco Interbank',\n",
       " u'Banco de la Nacion',\n",
       " u'Caja Palta',\n",
       " u'Interbank',\n",
       " u'Mi Banco',\n",
       " u'Mibanco',\n",
       " u'Scotiabank'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rechecker_names(dict_names):\n",
    "    \"\"\"creates a set of names from dict values\"\"\"\n",
    "    checking_list = []\n",
    "    for item in dict_names.values():\n",
    "        checking_list.extend(item)\n",
    "    return set(checking_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Azteca': [],\n",
       " 'BBVA': [u'BBVA'],\n",
       " 'BCP': [u'BCP'],\n",
       " 'BIF': [u'Banbif', u'BanBif'],\n",
       " 'Banco de la Nacion': [u'Banco de la Nacion'],\n",
       " 'Citibank': [],\n",
       " 'Compartamos': [],\n",
       " 'Edyficar': [],\n",
       " 'Falabella': [u'Banco Falabella'],\n",
       " 'Financiero': [u'Banco Financiero'],\n",
       " 'GNB': [],\n",
       " 'Interbank': [u'Banco Interbank', u'Interbank'],\n",
       " 'Mi Banco': [u'Mibanco', u'Mi Banco'],\n",
       " 'Others': [u'Caja Palta'],\n",
       " 'Scotiabank': [u'Scotiabank']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bank Clean    \n",
    "bank_dict = {\n",
    "        \"BBVA\": [],\n",
    "        \"BCP\": [],\n",
    "        \"Banco de la Nacion\": [],\n",
    "        \"Interbank\": [],\n",
    "        \"GNB\": [],\n",
    "        \"Falabella\": [],\n",
    "        \"Azteca\": [],\n",
    "        \"Mi Banco\": [],\n",
    "        \"Financiero\": [],\n",
    "        \"Scotiabank\": [],\n",
    "        \"Edyficar\": [],\n",
    "        \"Citibank\": [],\n",
    "        \"BIF\": [],\n",
    "        \"Compartamos\": [],\n",
    "        \"Others\": []\n",
    "    }\n",
    "\n",
    "for bank in bank_names:\n",
    "    if bank:\n",
    "        if \"continental\" in bank.lower() or \"bbva\" in bank.lower():\n",
    "            bank_dict[\"BBVA\"].append(bank)\n",
    "        elif (\"banco\" in bank.lower() and \"credito\" in bank.lower()) or (\"bcp\" in bank.lower()):\n",
    "            bank_dict[\"BCP\"].append(bank)\n",
    "        elif \"mibanco\" in bank.lower().replace(\" \", \"\"):\n",
    "            bank_dict[\"Mi Banco\"].append(bank)\n",
    "        elif (\"interbank\" in bank.lower() or \"internacional\" in bank.lower() \n",
    "              or \"intel\" in bank.lower()):\n",
    "            bank_dict[\"Interbank\"].append(bank)\n",
    "        elif \"nacion\" in bank.lower() or \"multired\" in bank.lower():\n",
    "            bank_dict[\"Banco de la Nacion\"].append(bank)\n",
    "        elif \"gnb\" in bank.lower():\n",
    "            bank_dict[\"GNB\"].append(bank)\n",
    "        elif \"financiero\" in bank.lower():\n",
    "            bank_dict[\"Financiero\"].append(bank)\n",
    "        elif \"azteca\" in bank.lower():\n",
    "            bank_dict[\"Azteca\"].append(bank)\n",
    "        elif \"falabella\" in bank.lower():\n",
    "            bank_dict[\"Falabella\"].append(bank)\n",
    "        elif \"scotiabank\" in bank.lower():\n",
    "            bank_dict[\"Scotiabank\"].append(bank)\n",
    "        elif \"edyficar\" in bank.lower():\n",
    "            bank_dict[\"Edyficar\"].append(bank)\n",
    "        elif \"citi\" in bank.lower():\n",
    "            bank_dict[\"Citibank\"].append(bank)\n",
    "        elif \"bif\" in bank.lower():\n",
    "            bank_dict[\"BIF\"].append(bank)\n",
    "        elif \"compartamos\" in bank.lower():\n",
    "            bank_dict[\"Compartamos\"].append(bank)\n",
    "        else:\n",
    "            bank_dict[\"Others\"].append(bank)   \n",
    "bank_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checking correctness of tag. Only choose those tags that we have already selected as important.\n",
    "def dict_no_other(dictio_items):\n",
    "    \"\"\"Takes away Other as the dict key so we can check only for the cases we have a solution.\"\"\"\n",
    "    selected_items = {item: value for item, value in dictio_items.items() if item != \"Others\"}\n",
    "    return selected_items\n",
    "selected_banks = dict_no_other(bank_dict)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'BanBif',\n",
       " u'Banco Interbank',\n",
       " u'Banco de la Nacion',\n",
       " 'BIF',\n",
       " u'Banco Financiero',\n",
       " u'BBVA',\n",
       " u'Mibanco',\n",
       " 'Falabella',\n",
       " u'Banbif',\n",
       " u'Scotiabank',\n",
       " u'BCP',\n",
       " u'Mi Banco',\n",
       " 'Financiero',\n",
       " u'Banco Falabella',\n",
       " u'Interbank']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inverting bank dictionary so mapping is easier\n",
    "def reverse_dict_items(dict_to_reverse):\n",
    "    \"\"\"Points the incorrect bank names (dict keys) to the correct entries (dict values)\"\"\"\n",
    "    reverse_dict = {item: value for value, items in dict_to_reverse.items() for item in items}\n",
    "    for value in set(reverse_dict.values()):\n",
    "        if value not in reverse_dict.keys():\n",
    "            reverse_dict[value] = value\n",
    "    return reverse_dict\n",
    "\n",
    "reverse_bank_dict = reverse_dict_items(selected_banks)\n",
    "curated_bank_set = reverse_bank_dict.keys()\n",
    "curated_bank_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing bank names\n",
    "def name_checker(element, reverse_dict, curated_list):\n",
    "    \"\"\"Normalize the name of the banks given a dictionary that points to the \n",
    "    corrected list (reverse dict) and the correct name of banks (curated_list)\"\"\"\n",
    "    type_amen, amen_name = get_amenity(element)\n",
    "    if amen_name and amen_name in curated_list:\n",
    "        amen_name = reverse_dict[amen_name]\n",
    "    return amen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def bank_tag_checker(element):\n",
    "    \"\"\"Checks if the amenity is correctly classified as a bank when there is no label given.\"\"\"\n",
    "    type_amen, amen_name = get_amenity(element)\n",
    "    if amen_name and amen_name in curated_bank_set and not type_amen:\n",
    "        type_amen = \"bank\"\n",
    "    return type_amen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bank_name_tag_checker(element):\n",
    "    \"\"\"Function for double checking if bank names and tags are Ok.\"\"\"\n",
    "    type_amen = bank_tag_checker(element)\n",
    "    amen_name = name_checker(element, reverse_bank_dict, curated_bank_set)\n",
    "    return type_amen, amen_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that our function is running as expected. As it is possible to see in the output below, the code is working as expected. For the banks that we have, it corrects the tag and labels it as a bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank Interbank\n",
      "bank Financiero\n",
      "bank Interbank\n",
      "bank Banco de la Nacion\n",
      "bank BBVA\n",
      "bank Falabella\n",
      "bank Mi Banco\n",
      "bank BCP\n",
      "bank Mi Banco\n",
      "bank Mi Banco\n",
      "bank BBVA\n",
      "bank Caja Palta\n",
      "bank Interbank\n",
      "bank Scotiabank\n",
      "bank BIF\n",
      "bank Scotiabank\n",
      "bank BCP\n",
      "bank Mi Banco\n",
      "bank BIF\n"
     ]
    }
   ],
   "source": [
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    tag, name = bank_name_tag_checker(element) \n",
    "    if tag == \"bank\" and name:\n",
    "        print tag, name\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking fast food names\n",
    "\n",
    "Here we check the main differences with fast food names. First we get the list of fast foods and check for different instances that refer to the same restaurant. We correct the list to modify these inconsistencies. We compare this list to the list of restaurants to check if there are restaurants that are double listed as restaurants and fast food. If this is the case, we include them in the fast food and take it off from the restaurant list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checking fast food restaurants\n",
    "fast_food_set = set()\n",
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    type_amen, amen_name = get_amenity(element)\n",
    "    if type_amen==\"fast_food\" or type_amen == \"restaurant\":\n",
    "            fast_food_set.add(amen_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None,\n",
       " u'Bembos',\n",
       " u'Bereket Doner',\n",
       " u'Caleo',\n",
       " u'Camila',\n",
       " u'Caravana',\n",
       " u'Cevicheria Micalo',\n",
       " u'Chifa Chang Shong',\n",
       " u'Chifa El Delicioso',\n",
       " u'Chifa Fu Hua',\n",
       " u'Chifa Guo',\n",
       " u'Chifa Hong Yi',\n",
       " u'Chifa La Muralla',\n",
       " u'Chifa Montesur',\n",
       " u'Chifa Sabor Cantones',\n",
       " u'Chifa Sax Say',\n",
       " u'Chifa Shin Wing',\n",
       " u'Chifa Shun De',\n",
       " u'Chifa Susu Huang',\n",
       " u'Chifa Tiansheng',\n",
       " u'Chifa Xing Lung',\n",
       " u'Chifa Yue Hua',\n",
       " u'Chifa Zhen',\n",
       " u'Chifa Zhen Yun',\n",
       " u'Combo Marino',\n",
       " u\"D'Candela\",\n",
       " u'Donde Alfredo',\n",
       " u'EDO Sushi Bar',\n",
       " u'Eco Pizza',\n",
       " u'El Mistiano',\n",
       " u'El Olimpico',\n",
       " u'El Peruano',\n",
       " u'El Sabor de Royer',\n",
       " u'House Pizzas',\n",
       " u'KFC',\n",
       " u'La Casa Del Pollo',\n",
       " u'La Diabla',\n",
       " u'La Panka',\n",
       " u'La Posada del Angel III',\n",
       " u'Las Canastas',\n",
       " u'Las Tinajas',\n",
       " u'Los Incas',\n",
       " u'Marcelino Pizza & Vino',\n",
       " u\"Mr. Noky's\",\n",
       " u'Mr. Sushi',\n",
       " u'Nautilius',\n",
       " u\"Norky's\",\n",
       " u'Norkys',\n",
       " u'OiSHii',\n",
       " u'Osharz',\n",
       " u'Palco Restobar',\n",
       " u'Perroquet',\n",
       " u'Pisa 2',\n",
       " u'Pizzeria El Italiano',\n",
       " u'Polleria Hikari',\n",
       " u'Pollos A La Lena',\n",
       " u'Pollos Chan',\n",
       " u\"Restaurant & Cevicheria D'Tomas\",\n",
       " u'Restaurant Buen Sabor',\n",
       " u'Restaurant Rosalia',\n",
       " u'Restaurant Vita',\n",
       " u\"Rocky's\",\n",
       " u\"Roky's\",\n",
       " u'Sansushito',\n",
       " u'Villa Chicken'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_food_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bank Clean    \n",
    "fast_food_dict = {\n",
    "        \"KFC\": [],\n",
    "        \"Bembos\": [],\n",
    "        \"Burger King\": [],\n",
    "        \"Pizza Hut\": [],\n",
    "        \"Papa John's\": [],\n",
    "        \"Popeye's\": [],\n",
    "        \"Norky's\": [],\n",
    "        \"Rocky's\": [],\n",
    "        \"Subway\": [],\n",
    "        \"McDonald's\": [],\n",
    "        \"China Wok\": [],\n",
    "        \"D'nnos Pizza\": [],\n",
    "        \"Telepizza\": [],\n",
    "        \"Tip Top\": [],\n",
    "        \"Dunkin Donuts\": [],\n",
    "        \"Domino's Pizza\": [],\n",
    "        \"Others\": []\n",
    "    }\n",
    "\n",
    "def renaming_fast_food(fast_food_list): \n",
    "    for fast_food in fast_food_list:\n",
    "        if fast_food:\n",
    "            if \"bembo\" in fast_food.lower():\n",
    "                fast_food_dict[\"Bembos\"].append(fast_food)\n",
    "            if \"kfc\" in fast_food.lower() or \"kentucky\" in fast_food.lower():\n",
    "                fast_food_dict[\"KFC\"].append(fast_food)\n",
    "            if \"burger king\" in fast_food.lower():\n",
    "                fast_food_dict[\"Burger King\"].append(fast_food)\n",
    "            if \"pizza hut\" in fast_food.lower():\n",
    "                fast_food_dict[\"Pizza Hut\"].append(fast_food)\n",
    "            if \"papa john\" in fast_food.lower():\n",
    "                fast_food_dict[\"Papa John's\"].append(fast_food)\n",
    "            if \"popeye\" in fast_food.lower():\n",
    "                fast_food_dict[\"Popeye's\"].append(fast_food)\n",
    "            if \"norky\" in fast_food.lower():\n",
    "                fast_food_dict[\"Norky's\"].append(fast_food)\n",
    "            if \"rocky\" in fast_food.lower() or \"roky\" in fast_food.lower() :\n",
    "                fast_food_dict[\"Rocky's\"].append(fast_food)\n",
    "            if \"subway\" in fast_food.lower():\n",
    "                fast_food_dict[\"Subway\"].append(fast_food)\n",
    "            if \"mcdonald\" in fast_food.lower() or \"mc don\" in fast_food.lower():\n",
    "                fast_food_dict[\"McDonald's\"].append(fast_food)\n",
    "            if \"china wok\" in fast_food.lower() or \"chinawok\" in fast_food.lower():\n",
    "                fast_food_dict[\"China Wok\"].append(fast_food)\n",
    "            if \"d'nnos \" in fast_food.lower() or \"dinnos\" in fast_food.lower():\n",
    "                fast_food_dict[\"D'nnos Pizza\"].append(fast_food)\n",
    "            if \"telepizz\" in fast_food.lower():\n",
    "                fast_food_dict[\"Telepizza\"].append(fast_food)\n",
    "            if \"tip top\" in fast_food.lower():\n",
    "                fast_food_dict[\"Tip Top\"].append(fast_food)\n",
    "            if \"dunki\" in fast_food.lower():\n",
    "                fast_food_dict[\"Dunkin Donuts\"].append(fast_food)\n",
    "            if \"domino\" in fast_food.lower():\n",
    "                fast_food_dict[\"Domino's Pizza\"].append(fast_food)\n",
    "            else:\n",
    "                fast_food_dict[\"Others\"].append(fast_food)\n",
    "    return fast_food_dict\n",
    "\n",
    "fast_food_dict = renaming_fast_food(fast_food_set)\n",
    "fast_food_dict[\"KFC\"].append(\"KfC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reversing restaurant names\n",
    "selected_rest = dict_no_other(fast_food_dict) \n",
    "reverse_rest_dict = reverse_dict_items(selected_rest)\n",
    "curated_rest = reverse_rest_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Changing Nonen and restaurant tag to fast food tag\n",
    "def fast_food_tag_checker(element):\n",
    "    type_amen, amen_name = get_amenity(element)\n",
    "    if amen_name and amen_name in curated_rest and type_amen=='restaurant':\n",
    "        type_amen = \"fast_food\"\n",
    "    elif amen_name and amen_name in curated_rest and not type_amen:\n",
    "        type_amen = \"fast_food\"\n",
    "    return type_amen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rest_name_tag_checker(element):\n",
    "    \"\"\"Function for checking if tag and names match specifications\"\"\"\n",
    "    type_amen = fast_food_tag_checker(element)\n",
    "    amen_name = name_checker(element, reverse_rest_dict, curated_rest)\n",
    "    return type_amen, amen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fast_food', \"Rocky's\")\n",
      "(u'restaurant', u'Villa Chicken')\n",
      "(u'restaurant', u'Palco Restobar')\n",
      "(u'restaurant', u'Chifa Hong Yi')\n",
      "(u'restaurant', u'Camila')\n",
      "(u'restaurant', u'Chifa Sabor Cantones')\n",
      "(u'restaurant', u'Marcelino Pizza & Vino')\n",
      "(u'restaurant', u'Las Canastas')\n",
      "(u'restaurant', u'Pollos A La Lena')\n",
      "(u'restaurant', None)\n"
     ]
    }
   ],
   "source": [
    "for element in get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    tag, amen_name = get_amenity(element)\n",
    "    if tag == \"fast_food\" or tag == \"restaurant\":\n",
    "        print rest_name_tag_checker(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing data to CSV\n",
    "\n",
    "Having written our functions that will help us clean the data, we pass the data to the appropriate CSV files, checking consistency.\n",
    "\n",
    "In order to pass our data to csv files, we use this code. We update it appropriately so the changes introduced by our defined functions are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "\n",
    "\n",
    "OSM_PATH = \"lima_peru.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "\n",
    "def tag_info_getter(tag, element):\n",
    "    \"\"\"Given an element it gives it a format so it can be passed to a csv file.\"\"\"\n",
    "    if PROBLEMCHARS.search(tag.attrib['k']):\n",
    "        return None\n",
    "    else:\n",
    "        dictio_tag = {}\n",
    "        dictio_tag['id'] = element.attrib['id']\n",
    "        k_value_split = tag.attrib['k'].split(\":\")\n",
    "        if len(k_value_split) > 1:\n",
    "            key_dict = \":\".join(k_value_split[:-1])\n",
    "            type_dict = k_value_split[-1]\n",
    "            dictio_tag['key'] = key_dict\n",
    "            dictio_tag['type'] = type_dict\n",
    "        else:\n",
    "            key_dict = tag.attrib['k']\n",
    "            type_dict = 'regular'\n",
    "            dictio_tag['key'] = tag.attrib['k']\n",
    "            dictio_tag['type'] = type_dict\n",
    "        if key_dict.startswith(\"addr\") or type_dict == \"street\":\n",
    "            dictio_tag['value'] = correcting_street_names(tag.attrib['v'])\n",
    "        norm_values = normalizing_attributes(tag.attrib['v'])\n",
    "        dictio_tag['value'] = norm_values\n",
    "        if tag.attrib['k'] == \"amenity\":\n",
    "            dictio_tag['value'] = bank_tag_checker(element)\n",
    "            if dictio_tag['value'] == tag.attrib['v']:\n",
    "                dictio_tag['value'] = fast_food_tag_checker(element)\n",
    "        if tag.attrib['k'] == \"name\":\n",
    "            dictio_tag['value'] = name_checker(element, reverse_bank_dict, curated_bank_set)\n",
    "            if dictio_tag['value'] == tag.attrib['v']:\n",
    "                dictio_tag['value'] = name_checker(element, reverse_rest_dict, curated_rest)\n",
    "        if tag.attrib['k'] == \"phone\":\n",
    "            list_phone = []\n",
    "            for phone in  phone_cleaner(tag.attrib['v']):\n",
    "                dict_phone = {}\n",
    "                dict_phone['id'] = element.attrib['id']\n",
    "                dict_phone['value'] = phone\n",
    "                dict_phone['key'] = tag.attrib['k']\n",
    "                dict_phone[\"type\"] = 'regular'\n",
    "                list_phone.append(dict_phone)\n",
    "            return list_phone\n",
    "        return dictio_tag\n",
    "\n",
    "def shape_element(element):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.attrib['id']\n",
    "        node_attribs['uid'] = element.attrib['uid']\n",
    "        node_attribs['user'] = element.attrib['user']\n",
    "        node_attribs['version'] = element.attrib['version']\n",
    "        node_attribs['lat'] = element.attrib['lat']\n",
    "        node_attribs['lon'] = element.attrib['lon']\n",
    "        node_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        node_attribs['changeset'] = element.attrib['changeset']\n",
    "        tags = [] \n",
    "        for tag in element.findall('tag'):\n",
    "            dictio_tag = tag_info_getter(tag, element)\n",
    "            if type(dictio_tag) == list:\n",
    "                for item in dictio_tag:\n",
    "                    tags.append(item)\n",
    "            else:\n",
    "                tags.append(dictio_tag)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs['id'] = element.attrib['id']\n",
    "        way_attribs['user'] = element.attrib['user']\n",
    "        way_attribs['version'] = element.attrib['version']\n",
    "        way_attribs['uid'] = element.attrib['uid']\n",
    "        way_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        way_attribs['changeset'] = element.attrib['changeset']\n",
    "        tags = []\n",
    "        way_nodes = []\n",
    "        counter = 0\n",
    "        for node in element.findall('nd'):\n",
    "            dictio_nodes = {}\n",
    "            dictio_nodes['id'] = element.attrib['id']\n",
    "            dictio_nodes['node_id'] = node.attrib['ref']\n",
    "            dictio_nodes['position'] = counter\n",
    "            counter += 1\n",
    "            way_nodes.append(dictio_nodes)\n",
    "        for tag in element.findall('tag'):\n",
    "            dictio_tag = tag_info_getter(tag, element)\n",
    "            if type(dictio_tag) == list:\n",
    "                for item in dictio_tag:\n",
    "                    tags.append(item)\n",
    "            else: \n",
    "                tags.append(dictio_tag)\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                try:\n",
    "                    if validate is True:\n",
    "                        if \"node_tags\" in el.keys():\n",
    "                            if None in el[\"node_tags\"]:\n",
    "                                el[\"node_tags\"].remove(None)\n",
    "                        elif \"way_tags\" in el.keys():\n",
    "                            if None in el[\"way_tags\"]:\n",
    "                                el[\"way_tags\"].remove(None)                        \n",
    "                        validate_element(el, validator)\n",
    "                except AttributeError:\n",
    "                    break\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "#process_map(OSM_FILE, validate=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:data_science_udemy]",
   "language": "python",
   "name": "conda-env-data_science_udemy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
