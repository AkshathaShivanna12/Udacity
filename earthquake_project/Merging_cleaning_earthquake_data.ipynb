{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake data\n",
    "\n",
    "This data was collected for the Udacity Exploratory Data Analysis course. In this Notebook we explain all the steps taken for merging the data. \n",
    "\n",
    "## Sources\n",
    "\n",
    "The final set uses these data sources. A description of each variable can be found in the code book **(make reference to code book here)**.\n",
    "\n",
    "1. **Eartquake data:** The data on earthquake was obtained from the Significance Earthquake Database [available here](http://www.ngdc.noaa.gov/nndc/struts/form?t=101650&s=1&d=1). This is  elaborated by the US National Oceanic and Atmospheric Administration. The dataset contains records of significant earthquakes and a series of variables related to their magnitude and damage. The dataset spans from 2150 BC till present. Check code book and page to see operational definition of significant earthquake.\n",
    "2. **World shape file:** We use the world shape file obtained in this [page](http://thematicmapping.org/downloads/world_borders.php). We use this shapefile for two reasons: first, it makes the process of merging the earthquake dataset more easy. The earthquake data contains a column with the full country name. However, for merging purposes, using the country name has problems, given that the same country may present slight or complete name variations in different data sets. The world shape file has the advantage to count with a ISO3 code for country. This is guaranteed to be a unique identifier per country. Second, with the shape file, we can also create maps to better visualize our results. In order to perform the merge, we use a spatial merge. More explanation to follow.\n",
    "3. **Penn World Tables:** The Penn World tables is a database that collects economic information from countries. The information runs from 1950, onwards. More information about the dataset is available [here](http://www.rug.nl/research/ggdc/data/pwt/).\n",
    "4. **Polity IV project:** This is a dataset that is widely used in political science research. It collects information about democratic/autocratic features for countries. The range of the dataset spans from 1800 to nowadays. From this dataset we extract the variable polityIV, which aims to be an score for describing how democratic a country is. Note that in order to merge this variable with our final data set we needed to carefully match the country names. All of the steps are explained below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with earthquake data\n",
    "\n",
    "First, we import the eartquake data. Afterwards, we decide on a certain conditions our dataset should have. For instance, we decide that we are going to concentrate only on earthquakes that happened from 1900 onwards. Also, we want complete our earthquake to have complete registries in terms of year, days and moths in which they occurred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading data into pandas\n",
    "earthquake_data = pd.read_csv(\"results.tsv\", sep = \"\\t\", dtype = {\"YEAR\": str, \"MONTH\": str, \n",
    "                                                                  \"DAY\": str, \"HOUR\": str, \"MINUTE\": str,\n",
    "                                                                 \"SECOND\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dropping incomplete date cases\n",
    "earthquake_data = earthquake_data.dropna(axis = 0, subset=[\"YEAR\", \"MONTH\", \"DAY\"])\n",
    "#selecting earthaquakes from 1900s onwards\n",
    "earthquake_data = earthquake_data.loc[earthquake_data.loc[:, \"YEAR\"].astype(int) >= 1900,  :]\n",
    "#creating a unique data\n",
    "earthquake_data[\"Date\"] = earthquake_data[[\"YEAR\", \"MONTH\", \"DAY\"]].apply(lambda x: \"-\".join(x), axis = 1)\n",
    "earthquake_data[\"Date\"] = pd.to_datetime(earthquake_data[\"Date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing world shape file\n",
    "world_shp = gpd.GeoDataFrame.from_file('/Users/Jose/Documents/Udacity/data_science/earthquake_project/world_borders/TM_WORLD_BORDERS-0.3.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging earthquake data with world shape file\n",
    "Next, we prepare our earthquake data for a spatial merge with the world shapefile. The world shapefile contains the geometries of each country in the world. On the other hand, the earthquake data has latitude and longitude coordinates for the earthquakes' location. The spatial join will look if the latitude and longitude fall within the boundaries of a country geometry and, if so, merge it appropriately. \n",
    "\n",
    "In order for the merge to yield appropriate results, we need to do the following: first, translate the latitude and longitude coordinates into their appropriate shape form, that is points. Then, we need to take out all the cases where one of these variables is missing. Otherwise, the join wil raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Changing type of LAT / LONG to numeric\n",
    "earthquake_data[[\"LONGITUDE\", \"LATITUDE\"]] = earthquake_data[[\"LONGITUDE\", \"LATITUDE\"]].apply(pd.to_numeric, errors='coerce')\n",
    "#Dropping missing LAT/LONG values\n",
    "earthquake_data = earthquake_data.dropna(axis = 0, subset=[\"LONGITUDE\", \"LATITUDE\"])\n",
    "#Transforming LAT/LONG to points\n",
    "earthquake_data[\"Points\"] = earthquake_data[[\"LONGITUDE\", \"LATITUDE\"]].apply(lambda row: Point(row[\"LONGITUDE\"], row[\"LATITUDE\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Passing earthquake data to a GeoPandas dataframe\n",
    "earthquake_data = gpd.GeoDataFrame(earthquake_data)\n",
    "#Declaring the coordinate system for the earthquake data \n",
    "earthquake_data.crs = world_shp.crs\n",
    "earthquake_data[\"geometry\"] = earthquake_data[\"Points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Performing spatial join\n",
    "joined_data = gpd.sjoin(earthquake_data, world_shp, how=\"right\", op='intersects')\n",
    "joined_data = joined_data.reset_index()\n",
    "#Dropping cases with no earthquakes\n",
    "joined_data.dropna(axis = 0, subset=[\"LATITUDE\", \"LONGITUDE\"], inplace=True)\n",
    "#Cleaning data set\n",
    "joined_data = joined_data.drop([\"index_left\", \"LAT\", \"LON\", \"POP2005\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Penn World Tables\n",
    "\n",
    "We use two variables from the Penn World Tables, GDP (expenditure side) at purchase parity prices and population. Note that this data set only runs from 1950 and it has a lot of missing for most of the countries in early years. As this is a yearly data, we make sure to merge on year and country. Note that the earthquake data gather so far has no fixed time span, as an earthquake could happen at any moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading PENN tables\n",
    "penn_tables = pd.read_excel(\"pwt90.xlsx\", sheetname=\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We select a subset of the data (rgdpe and pop)\n",
    "penn_tables = penn_tables.loc[:, [\"countrycode\", \"country\", \"year\", \"rgdpe\", \"pop\"]]\n",
    "#We drop all of the empty registries\n",
    "penn_tables = penn_tables.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countrycode     object\n",
       "country         object\n",
       "year             int64\n",
       "rgdpe          float64\n",
       "pop            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking data types for merging purposes\n",
    "penn_tables.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Changing year type for merging purposes (matching of types)\n",
    "joined_data[\"YEAR\"] = joined_data[\"YEAR\"].astype(int)\n",
    "#Right merge of both data sets (we want to keep earthquake records \n",
    "##even though there is no complete data in Penn World Tables)\n",
    "earth_penn = pd.merge(penn_tables, joined_data, left_on = [\"year\", \"countrycode\"], right_on = [\"YEAR\", \"ISO3\"], how = \"right\", indicator = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checking merge results for merged data\n",
    "earth_penn[(earth_penn[\"_merge\"] == \"right_only\") & (earth_penn[\"YEAR\"] > 1950)] \n",
    "#dropping \"_merge\" variable used to check if merge was appropriately carried out\n",
    "earth_penn.drop(\"_merge\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Polity IV data\n",
    "The polity IV data uses a particular country code identifier. Therefore, we needed to make sure to find the appropriate correspondance between that country code and the ISO3 country code used in our dataset. \n",
    "\n",
    "For this purpose, we take the following steps:\n",
    "1. We import the country codes data sets from the Polity IV data. We also subset our earthquake data and only take the country codes variables. This will make the correspondance process lighter, as we will only be working with the variables we are actually interested in.\n",
    "2. We use the following strategy: first we merge on country codes of both data sets and then on country names. \n",
    "3. **merging on coutry codes **. Problems found: a proper matching case between ISO3 code and PolityIV country code exists, however the mathching is false. For example, while there was a match for SLV in both data sets, this was not appropriate. In ISO3 this corresponds to El Salvador and in PolityIV to Slovenia. We take appropriate steps to check that the corresponding match is found.\n",
    "4. **merging on names **. Once taken out all of those cases where an appropriate country code matched existed, we use country names to check if there are more matches available. Luckily, all the matches produced here are rightly done so no further correction is needed.\n",
    "5. **non merged countries**. Inspecting the difference between our current matched countries and the earthquake data set we found that there are still 8 countries for which there is no appropriate match. For this cases we performed a search in the PolityIV country names to see if they contain a patter related to that country. In the end we got to find appropriate matches for 5 countries. For this countries we manually select their corresponding PolityIV identifier and add it to our final correspondance list. The other countries, namely Puerto Rico, Palestine and Serbia are not found in the PolityIV data. This may have to with the working definition of country implied in both datasets.\n",
    "\n",
    "Having checked that we have the appropriate correspondance for countries between polityIV and our earthquake data we perform the merge to get our final dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading in country codes\n",
    "c_codes = pd.read_csv(\"COW country codes.csv\")\n",
    "c_codes = c_codes.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'StateAbb', u'CCode', u'StateNme'], dtype='object')"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_codes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading in country codes for our earthquake dataset\n",
    "country_checker = earth_penn[[\"ISO3\", \"NAME\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merging both datasets on country codes\n",
    "first_merge = pd.merge(c_codes, country_checker, left_on=\"StateAbb\", right_on=\"ISO3\", how=\"outer\", indicator=True)\n",
    "#Non correspondance for earthquake data countries finding names in earthquake data\n",
    "resolving_names_iso3 = country_checker.query('NAME == [\"Austria\", \"Montenegro\", \"Slovenia\"]')\n",
    "#Finding names in polityIV country data\n",
    "resolving_names_cow = first_merge.query('StateNme == [\"Montenegro\", \"Slovenia\"]')[[\"StateAbb\",\"StateNme\"]]\n",
    "#Generating our country correspondance dataframe\n",
    "trans_iso_cow = pd.merge(resolving_names_iso3, resolving_names_cow, left_on =\"NAME\", right_on=\"StateNme\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Working on non correspondances for earthquake data set\n",
    "#Finding corresponding names in PolityIV country identifiers\n",
    "resolving_names_cow =  c_codes[(c_codes.StateNme.str.contains(\"Salva\")) | \n",
    "                               (c_codes.StateNme.str.contains(\"Mongo\")) | \n",
    "                                (c_codes.StateNme.str.contains(\"Austral\"))][[\"StateAbb\",\"StateNme\"]]\n",
    "resolving_names_iso3 = country_checker.query('NAME == [\"El Salvador\", \"Mongolia\", \"Australia\"]')\n",
    "#Adding ready country codes correspondances to previously created dataframe\n",
    "trans_iso_cow = pd.concat((trans_iso_cow, pd.merge(resolving_names_iso3, \n",
    "                                resolving_names_cow, left_on =\"NAME\", right_on=\"StateNme\")), \n",
    "          ignore_index= True)\n",
    "trans_iso_cow = trans_iso_cow.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Adding cases to our correspondance list for which merging was appropriately performed\n",
    "both_cases = first_merge[first_merge[\"_merge\"] == \"both\"]\n",
    "#Taking out bad cases for which solution was found\n",
    "good_both_cases = both_cases.loc[~both_cases[\"ISO3\"].isin(trans_iso_cow[\"ISO3\"]),:][[\"StateAbb\",\"StateNme\", \"ISO3\", \"NAME\"]]\n",
    "#Adding to our final correspondance dataframe\n",
    "trans_iso_cow = pd.concat((trans_iso_cow, good_both_cases), \n",
    "          ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merging on names\n",
    "right_only = first_merge.loc[first_merge[\"_merge\"] == \"right_only\", [\"NAME\", \"ISO3\"]]\n",
    "right_merge_cc = pd.merge(right_only, c_codes, left_on = \"NAME\", right_on = \"StateNme\" )\n",
    "right_merge_cc = right_merge_cc.loc[:, [\"ISO3\", \"NAME\", \"StateAbb\", \"StateNme\"]]\n",
    "#Adding to our final correspondance list after checking appropriate correspondance\n",
    "trans_iso_cow = pd.concat((trans_iso_cow, right_merge_cc), \n",
    "          ignore_index= True)\n",
    "trans_iso_cow = trans_iso_cow.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO3</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>MKD</td>\n",
       "      <td>The former Yugoslav Republic of Macedonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>MMR</td>\n",
       "      <td>Burma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>SRB</td>\n",
       "      <td>Serbia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>TZA</td>\n",
       "      <td>United Republic of Tanzania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>PSE</td>\n",
       "      <td>Palestine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>LCA</td>\n",
       "      <td>Saint Lucia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>LBY</td>\n",
       "      <td>Libyan Arab Jamahiriya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>PRI</td>\n",
       "      <td>Puerto Rico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ISO3                                       NAME\n",
       "952   MKD  The former Yugoslav Republic of Macedonia\n",
       "953   MMR                                      Burma\n",
       "1161  SRB                                     Serbia\n",
       "1285  TZA                United Republic of Tanzania\n",
       "1688  PSE                                  Palestine\n",
       "1882  LCA                                Saint Lucia\n",
       "2169  LBY                     Libyan Arab Jamahiriya\n",
       "2173  PRI                                Puerto Rico"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking which countries in earthquake do not have their appropriate correspondance\n",
    "lacking_countries = country_checker.loc[~country_checker[\"ISO3\"].isin(trans_iso_cow[\"ISO3\"]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finding correct correspondances for those countries in PolityIV country data\n",
    "pattern = r'Luc|Libya|Tanza|anma|Mac'\n",
    "finding_matches = c_codes.loc[(c_codes.StateNme.str.contains(pattern)),:]\n",
    "finding_matches.loc[:,\"right_index_match\" ] = pd.Series([1882, 952, 1285, 2169, 953], index=finding_matches.index)\n",
    "concat_matched = pd.merge(finding_matches, lacking_countries, left_on=\"right_index_match\", right_index=True).loc[:, [\"ISO3\", \"NAME\", \"StateAbb\", \"StateNme\"]]\n",
    "##Adding manually matched index to final dataset.\n",
    "trans_iso_cow = pd.concat((trans_iso_cow, concat_matched), \n",
    "          ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_translate = trans_iso_cow.loc[:, [\"ISO3\", \"StateAbb\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earth_penn = pd.merge(earth_penn, country_translate, on=\"ISO3\", how = \"left\", indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "earth_penn = earth_penn.drop([\"_merge\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading PolityIV dataset\n",
    "polityIV = pd.read_excel(\"p4v2015.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polityIV = polityIV.loc[: ,[\"scode\", \"country\", \"year\", \"polity2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "earthquake_data = pd.merge(earth_penn, polityIV, \n",
    "         left_on = [\"StateAbb\", \"YEAR\"], \n",
    "         right_on = [\"scode\", \"year\"], \n",
    "         how = \"left\",\n",
    "        indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clening data_set. Selecting variables we'll work with\n",
    "drop_columns = ['countrycode', 'country_x', 'year_x', 'I_D', \n",
    " 'COUNTRY', 'STATE', 'LOCATION_NAME','REGION_CODE',\n",
    "'StateAbb', 'scode',  'country_y', 'year_y', '_merge']\n",
    "earthquake_data = earthquake_data.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geom_data = earthquake_data[['geometry', 'ISO3', 'YEAR']]\n",
    "geom_data.to_csv(\"geometry_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "earthquake_data = earthquake_data.drop(\"geometry\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "earthquake_data.to_csv(\"earthquake_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:data_science_udemy]",
   "language": "python",
   "name": "conda-env-data_science_udemy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
